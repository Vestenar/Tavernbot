import requests
from json import loads
from random import randint
from bs4 import BeautifulSoup


def get_currencies():
    currencies = []
    cbcurs = requests.get('https://www.cbr-xml-daily.ru/daily_json.js')
    currency_cb = loads(cbcurs.text)
    currencies.append('1 USD üíµ:         {} —Ä—É–±–ª–µ–π'.format(currency_cb['Valute']['USD']['Value']))
    currencies.append('1 EUR üí∂:         {} —Ä—É–±–ª–µ–π'.format(currency_cb['Valute']['EUR']['Value']))
    godcurs = requests.get('https://godville.net/news')
    soup = BeautifulSoup(godcurs.text, 'lxml')
    kotirs = soup.find('div', {'class': 'rate clearfix'})
    currencies.append('1 –±–æ—Å—Å–∫–æ–∏–Ω üßø:  ' + kotirs.contents[5].contents[0] + 'üí∞')
    currencies.append('1 –∫–∏—Ä–ø–∏—á üß±:     ' + kotirs.contents[9].contents[0] + 'üí∞')
    currencies.append('1 –∏–Ω–≤–∞–π—Ç üîñ:     ' + kotirs.contents[13].contents[0] + 'üí∞')
    cbcurs = None
    godcurs = None
    soup = None
    kotirs = None
    return '–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç:\n\n' + '\n'.join(currencies)


def get_story():
    url = 'http://lolstory.ru/story/'
    antimat = '–í–æ–∑–º–æ–∂–Ω–∞ –Ω–µ–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞. ' \
              '–ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Å–ª–æ–≤–æ –æ—Ç–∫–ª—é—á–∏—Ç–µ —Ü–µ–Ω–∑–æ—Ä –≤ –Ω–∏–∑—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –æ–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É. '
    MAXLEN = 1000
    for _ in range(10):
        number = randint(1, 14950)
        get_url = url + str(number)
        resp = requests.get(get_url)
        if resp.status_code != 200:
            return '–£—Å—Ç–∞–ª —è —Å–µ–≥–æ–¥–Ω—è, –¥–∞ –∏ –ø–∞–º—è—Ç—å —É–∂–µ –Ω–µ —Ç–∞'

        soup = BeautifulSoup(resp.content, 'lxml')
        story = soup.find('div', {'class': 'post-text'})
        if story and len(story.text) <= MAXLEN:
            return story.text.strip().replace('\n\r', '\n').replace(antimat, '')
        soup = None
        story = None
    return '–ß—Ç–æ-—Ç–æ –≤ –ø–∞–º—è—Ç–∏ –≤—Å–ø–ª—ã–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏, –Ω–µ —Ö–æ—á–µ—Ç—Å—è –≤–∞—Å –∏–º–∏ —É—Ç—Ä—É–∂–¥–∞—Ç—å'


def get_movie():    # TODO –¥–æ–¥–µ–ª–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –ø–æ–¥ ajax –∏–ª–∏ –Ω–∞–π—Ç–∏ –¥—Ä—É–≥–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫
    url = 'https://www.kinomania.ru/top/films'
    headers = {'User-Agent': 'Mozilla/5.0', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'}
    session = requests.Session()
    resp = session.get(url, headers=headers)
    if resp.status_code != 200:
        return '–î–∞ –∫–∞–∫–∏–µ —Ñ–∏–ª—å–º—ã? –†–∞–±–æ—Ç–∞—Ç—å –Ω—É–∂–Ω–æ'
    cookies = requests.utils.cookiejar_from_dict(requests.utils.dict_from_cookiejar(session.cookies))
    headers = {'User-Agent': 'Mozilla/5.0',
               'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
               'X-Requested-With': 'XMLHttpRequest'}
    url = 'https://www.kinomania.ru/top/films?handler=search'
    resp = session.post(url, headers=headers, cookies=cookies)
    session.close()
    if resp.status_code != 200:
        return '–î–∞ –∫–∞–∫–∏–µ —Ñ–∏–ª—å–º—ã? –†–∞–±–æ—Ç–∞—Ç—å –Ω—É–∂–Ω–æ'
    data = loads(resp.text)
    resp = None
    position = randint(0, 99)
    data = data[position]

    return '–ù—É—É—É, –Ω–∞–ø—Ä–∏–º–µ—Ä... "{}".\n–†–µ–π—Ç–∏–Ω–≥: {}. –ü–æ–∑–∏—Ü–∏—è –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ: {}.'.format(data['name_ru'],
                                                                                  data['rate'], position + 1)


def get_recepie():      # TODO –Ω–∞–π—Ç–∏ –¥—Ä—É–≥–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫, –Ω–µ —Ç–∞–∫–æ–π –∫–æ—Ä—è–≤—ã–π
    page = randint(1, 49)
    url = 'https://www.povarenok.ru/recipes/category/20/~{}/?sort=rating&order=desc'.format(page)
    resp = requests.get(url)
    if resp.status_code != 200:
        return '–ê —è —É–∂–µ –≤—Å–µ –∑–∞–º–µ—à–∞–ª –∏ —Ç–∞–∫', '', None
    soup = BeautifulSoup(resp.text, 'lxml')
    recepies = soup.find_all('article', {'class': 'item-bl'})

    showrec = randint(0, 14)
    recepie_link = recepies[showrec].contents[1].contents[1]['href']
    recepie_name = recepies[showrec].contents[3].contents[1].text
    recepie_picture = recepies[showrec].contents[5].contents[1].contents[1]['src']

    resp_chosen = requests.get(recepie_link)
    if resp.status_code != 200:
        return '–ê —è —É–∂–µ –≤—Å–µ –∑–∞–º–µ—à–∞–ª –∏ —Ç–∞–∫', '', None

    soup = BeautifulSoup(resp_chosen.text, 'lxml')
    ingridients = soup.find('div', {'class': 'ingredients-bl'})

    ingr_list = ['–ù–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è:']
    for i in range(1, len(ingridients.contents[1].contents), 2):
        name = ingridients.contents[1].contents[i].contents[1].contents[1].text.strip()
        if len(ingridients.contents[1].contents[i].contents[1].contents) > 3:
            quantity = ingridients.contents[1].contents[i].contents[1].contents[3].text.strip()
        else:
            quantity = ''
        ingr_list.append(name + ':  ' + quantity)
    ingridients = '\n'.join(ingr_list)

    preparation = soup.find_all('div', {'class': 'cooking-bl'})
    text = ''
    if preparation:
        for point in preparation:
            text += point.text.strip() + '\n'
    else:
        preparation = soup.find('article', {'class': 'item-bl item-about'})
        for i in range(34, 38):
            if preparation.contents[1].contents[i].name == 'div':
                text = preparation.contents[1].contents[i].text
                break

    first_text = '{}\n\n{}'.format(recepie_name, ingridients)
    soup = None
    resp = None
    resp_chosen = None
    preparation = None
    ingridients = None
    return first_text, text, recepie_picture


def get_football(league, group=None):
    parts, games, url = [], [], ''
    if league == 'champ':
        url = 'https://terrikon.com/champions-league/'
        # name = '–ß–µ–º–ø–∏–æ–Ω–æ–≤'
        parts = ['1/8 —Ñ–∏–Ω–∞–ª–∞', '1/4 —Ñ–∏–Ω–∞–ª–∞', '1/2 —Ñ–∏–Ω–∞–ª–∞', '–§–∏–Ω–∞–ª']
    elif league == 'euro':
        url = 'https://terrikon.com/europa-league/'
        # name = '–ï–≤—Ä–æ–ø—ã'
        parts = ['1/8 —Ñ–∏–Ω–∞–ª–∞', '1/4 —Ñ–∏–Ω–∞–ª–∞', '1/2 —Ñ–∏–Ω–∞–ª–∞', '–§–∏–Ω–∞–ª']
    elif league == 'konf':
        url = 'https://terrikon.com/conference-league/'
        parts = ['1/8 —Ñ–∏–Ω–∞–ª–∞', '1/4 —Ñ–∏–Ω–∞–ª–∞', '1/2 —Ñ–∏–Ω–∞–ª–∞', '–§–∏–Ω–∞–ª']
    elif league == 'world':
        url = 'https://terrikon.com/worldcup-2022/'
        parts = ['1/8 —Ñ–∏–Ω–∞–ª–∞', '1/4 —Ñ–∏–Ω–∞–ª–∞', '1/2 —Ñ–∏–Ω–∞–ª–∞', '–§–∏–Ω–∞–ª']
    headers = {'User-Agent': 'Mozilla/5.0', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'}
    session = requests.Session()
    resp = session.get(url, headers=headers)
    session.close()
    if resp.status_code != 200:
        return '–ö–∞–∫–æ–π —Ç–∞–∫–æ–π —Ñ—É—Ç–±–æ–ª –µ—â–µ?'
    soup = BeautifulSoup(resp.text, 'lxml')
    name = soup.find('h1').text
    # table = f'–õ–∏–≥–∞ {name} 2020/21. '
    table = name + ' \n'
    # parts = {3: '1/8 —Ñ–∏–Ω–∞–ª–∞', 2: '1/4 —Ñ–∏–Ω–∞–ª–∞', 1: '1/2 —Ñ–∏–Ω–∞–ª–∞', 0: '–§–∏–Ω–∞–ª'}

    if not group:
        for part in parts:
            # games = soup.find_all('table', {'class': 'gameresult'})[part]
            games = soup.select_one(f'h2:-soup-contains("{part}")')
            if not games:
                return table + f'–∏–≥—Ä—ã {parts[0]} –µ—â–µ –Ω–µ –Ω–∞—á–∞–ª–∏—Å—å'
            games = games.find_next('table')
            if '-:-' in games.text:
                table += part + '\n\n'
                break
    else:
        table += f'–ì—Ä—É–ø–ø–∞ {group}\n\n'
        games = soup.select_one(f'h2:-soup-contains("{group}")').find_next('table', {'class': 'gameresult'})
        if not games:
            return table + f'–ì—Ä—É–ø–ø–∞ {group} –≤ —Ç—É—Ä–Ω–∏—Ä–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'

    for game in games:
        if game != '\n':
            teams = game.find_all('td', {'class': 'team'})
            score = game.find_all('td', {'class': 'score'})
            date = game.find_all('td', {'class': 'date'})
            table += f'{teams[0].text:^15}-{teams[1].text:^15}\n'
            table += f'{date[0].text:^15}{score[0].text:^15}\n'
            table += f'{"-" * 31}\n'
    return table


def get_promo():
    import re
    url = 'https://www.goha.ru/genshin-impact-aktualnye-promokody-na-mart-2021-goda-lWOPnm'
    headers = {'User-Agent': 'Mozilla/5.0', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'}
    session = requests.Session()
    resp = session.get(url, headers=headers)
    soup = BeautifulSoup(resp.text, 'lxml')
    codes = soup.find('strong', text=re.compile('–†–∞–±–æ—á–∏–µ –ø—Ä–æ–º–æ–∫–æ–¥—ã'))
    codes = codes.findNext('ul')
    codes = codes.find_all('strong')
    with open('genshinpromo.txt', 'r') as promofile:
        known_codes = promofile.readlines()
        known_codes = [code.rstrip() for code in known_codes]
    codes_for_sent = []
    for code in codes:
        if code.text.strip() not in known_codes:
            codes_for_sent.append(code.text.strip())
    text = '\n'.join(codes_for_sent)
    with open('genshinpromo.txt', 'a') as promofile:
        for code in codes_for_sent:
            promofile.write(code + '\n')
    return text


if __name__ == '__main__':
    print(get_football('champ', 'A'))
    # print(get_football('euro'))
    # print(get_promo())
